<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>TEASEL: a Transformer-Based Speech-Prefixed Language Model </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">M. Arjmand</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-category">Publications</div>
<div class="menu-item"><a href="teasel.html">TEASEL</a></div>
<div class="menu-category">Opensource Porojects</div>
<div class="menu-item"><a href="https://github.com/Michaelvll/DeepCCA">Deep&nbsp;CCA&nbsp;Pytorch</a></div>
<div class="menu-item"><a href="https://github.com/arminarj/DeepGCCA-pytorch">Deep&nbsp;GCCA&nbsp;Pytorch</a></div>
<div class="menu-category">External Links</div>
<div class="menu-item"><a href="https://github.com/arminarj">Github</a></div>
<div class="menu-item"><a href="https://www.linkedin.com/in/mehdi-arjmand-6aa607133/ ">LinkedIn</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>TEASEL: a Transformer-Based Speech-Prefixed Language Model </h1>
</div>
<table class="imgtable"><tr><td>
<img src="teasel_fig_1.png" alt="alt text" width="510px" height="200px" />&nbsp;</td>
<td align="left"><ul>
<li><p><b>Mehdi Arjmand</b>, Mohammad Javad Dousti, and Hadi Moradi, <b>&ldquo;<i>TEASEL: a Transformer-Based Speech-Prefixed Language Model</i>&rdquo;</b>, arXiv preprint arXiv:2109.05522, 2021
<br /> <a href="teasel.html">Website</a> | <a href="https://arxiv.org/abs/2109.05522.pdf">Arxiv</a> | <a href="https://github.com/arminarj/teasel">Code</a></p>
</li>
</ul>
</td></tr></table>
<h2>Abstract</h2>
<p>Multimodal language analysis is a burgeoning field of NLP that aims to simultaneously model a speaker's words, acoustical annotations, and facial expressions. In this area, <br /> 
lexicon features usually outperform other modalities because they are pre-trained on large corpora via Transformer-based models. Despite their strong performance, training <br /> 
a new <i>self-supervised learning</i> (SSL) Transformer on any modality is not usually attainable due to insufficient data,  which is the case in multimodal language learning. <br /> 
This work proposes a <i>Transformer-Based Speech-Prefixed Language Model</i> called TEASEL to approach the mentioned constraints without training a complete Transformer model. <br /> 
teasel model includes speech modality as a dynamic prefix besides the textual modality compared to a conventional language model. This method exploits a conventional <br /> 
pre-trained language model as a cross-modal Transformer model. We evaluated TEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset. Extensive experiments <br /> 
show that our model outperforms unimodal baseline language models by 4% and outperforms the current <i>state-of-the-art</i> (SoTA) model by 1% in F1-score. Additionally, our proposed <br /> 
method is 72% smaller than the SoTA model.</p>
<table class="imgtable"><tr><td>
<img src="teasel_fig_2.png" alt="alt text" width="537px" height="403px" />&nbsp;</td>
<td align="left"></td></tr></table>
<h2>Citation</h2>
<div class="infoblock">
<div class="blocktitle"></div>
<div class="blockcontent">
<p>@misc{arjm2021teasel,<br />
title={TEASEL: A Transformer-Based Speech-Prefixed Language Model},<br />
author={Mehdi Arjmand and Mohammad Javad Dousti and Hadi Moradi}, <br />
year={2021},<br />
eprint={2109.05522},<br />
archivePrefix={arXiv},<br />
primaryClass={cs.CL}<br />
}</p>
</div></div>
<div id="footer">
<div id="footer-text">
Page generated 2021-09-14 09:04:36 +0430, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
